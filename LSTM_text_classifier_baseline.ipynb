{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hishamp3/Thesis-NLP/blob/main/LSTM_text_classifier_baseline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "N0l2k3JMkAHN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ef40c69-ea5a-41d4-9fa8-3adc20e80b57"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m524.1/524.1 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m54.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.6/311.6 kB\u001b[0m \u001b[31m29.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m52.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m440.8/440.8 kB\u001b[0m \u001b[31m35.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m58.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "pydantic 2.1.1 requires typing-extensions>=4.6.1, but you have typing-extensions 4.5.0 which is incompatible.\n",
            "pydantic-core 2.4.0 requires typing-extensions!=4.7.0,>=4.6.0, but you have typing-extensions 4.5.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -q -U \"tensorflow-text\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.layers import Embedding"
      ],
      "metadata": {
        "id": "1JUe_sy1ttKZ"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv(\"./sample_data/fake reviews dataset.csv\",usecols=[\"text_\",\"label\"])"
      ],
      "metadata": {
        "id": "Jrosfvb86C-C"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['fake']=df['label'].apply(lambda x: 1 if x=='CG' else 0)"
      ],
      "metadata": {
        "id": "kcmAMJN86A9M"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(df['text_'],df['fake'], stratify=df['fake'])\n",
        "X_train.head(4)"
      ],
      "metadata": {
        "id": "xix7-E2QnHXJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c076db3b-712a-4387-dc9b-195d30eb7265"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "31903    This is the most psychotic book ever written! ...\n",
              "15560    This is Mudd.  Backlash, an epidemic of violen...\n",
              "17560    My absolute favorite thing about my new house....\n",
              "23522    I bought this product a few weeks ago and it w...\n",
              "Name: text_, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = X_train.to_frame().join(y_train)"
      ],
      "metadata": {
        "id": "kNiVkwLQ3VJE"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train_dataset = pd.concat([X_train,y_train])"
      ],
      "metadata": {
        "id": "Zba237km0J0b"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining pre-processing parameters\n",
        "max_len = 50\n",
        "trunc_type = 'post'\n",
        "padding_type = 'post'\n",
        "oov_tok = '<OOV>' # out of vocabulary token\n",
        "vocab_size = 500"
      ],
      "metadata": {
        "id": "tonoxOoixCQF"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.callbacks import EarlyStopping"
      ],
      "metadata": {
        "id": "29Yy0JpKE37L"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer(num_words = vocab_size,\n",
        "                      char_level = False,\n",
        "                      oov_token = oov_tok)\n",
        "tokenizer.fit_on_texts(X_train)"
      ],
      "metadata": {
        "id": "oHOkDRf1D9_E"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_index = tokenizer.word_index\n",
        "total_words = len(word_index)\n",
        "print(total_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GWN3pxYXFB7r",
        "outputId": "f7f35255-8499-49e5-9964-3d10c92b62ea"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "37941\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training_sequences = tokenizer.texts_to_sequences(X_train)\n",
        "training_padded = pad_sequences(training_sequences,\n",
        "                                maxlen = max_len,\n",
        "                                padding = padding_type,\n",
        "                                truncating = trunc_type)"
      ],
      "metadata": {
        "id": "86oIu2PFFIDL"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "testing_sequences = tokenizer.texts_to_sequences(X_test)\n",
        "testing_padded = pad_sequences(testing_sequences,\n",
        "                               maxlen = max_len,\n",
        "                               padding = padding_type,\n",
        "                               truncating = trunc_type)"
      ],
      "metadata": {
        "id": "2UXIvtH-FSvb"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Shape of training tensor: ', training_padded.shape)\n",
        "print('Shape of testing tensor: ', testing_padded.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w89MxyNnFYR8",
        "outputId": "50e90902-4422-4fae-81e3-5bed24b321d8"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of training tensor:  (30324, 50)\n",
            "Shape of testing tensor:  (10108, 50)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Modeling\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, GRU, Dense, Embedding, Dropout, GlobalAveragePooling1D, Flatten, SpatialDropout1D, Bidirectional"
      ],
      "metadata": {
        "id": "zw48IdndFkJ8"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = 500\n",
        "embedding_dim = 16\n",
        "drop_value = 0.2\n",
        "n_dense = 24"
      ],
      "metadata": {
        "id": "F3NGLJD5JJkI"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define parameter\n",
        "n_lstm = 128\n",
        "drop_lstm = 0.2\n",
        "# Define LSTM Model\n",
        "model1 = Sequential()\n",
        "model1.add(Embedding(vocab_size, embedding_dim, input_length=max_len))\n",
        "model1.add(SpatialDropout1D(drop_lstm))\n",
        "model1.add(LSTM(n_lstm, return_sequences=False))\n",
        "model1.add(Dropout(drop_lstm))\n",
        "model1.add(Dense(1, activation='sigmoid'))"
      ],
      "metadata": {
        "id": "3m24juVAFdb7"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model1.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uIa-07CrGJ6P",
        "outputId": "1fc5062f-6fb5-4322-cff2-d3d505989486"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 50, 16)            8000      \n",
            "                                                                 \n",
            " spatial_dropout1d (SpatialD  (None, 50, 16)           0         \n",
            " ropout1D)                                                       \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, 128)               74240     \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 128)               0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1)                 129       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 82,369\n",
            "Trainable params: 82,369\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model1.compile(loss = 'binary_crossentropy',\n",
        "               optimizer = 'adam',\n",
        "               metrics = ['accuracy'])"
      ],
      "metadata": {
        "id": "Ci676SrjHejP"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 5\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=2)\n",
        "history = model1.fit(training_padded,\n",
        "                     y_train,\n",
        "                     epochs=num_epochs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YMLPHACcHjp8",
        "outputId": "10114ce0-5c1e-4375-d910-a03a07e82c7f"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7a404ef93490> and will run it as-is.\n",
            "Cause: Unable to locate the source code of <function Model.make_train_function.<locals>.train_function at 0x7a404ef93490>. Note that functions defined in certain environments, like the interactive Python shell, do not expose their source code. If that is the case, you should define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.experimental.do_not_convert. Original error: could not get source code\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7a404ef93490> and will run it as-is.\n",
            "Cause: Unable to locate the source code of <function Model.make_train_function.<locals>.train_function at 0x7a404ef93490>. Note that functions defined in certain environments, like the interactive Python shell, do not expose their source code. If that is the case, you should define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.experimental.do_not_convert. Original error: could not get source code\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:AutoGraph could not transform <bound method _BaseOptimizer._update_step_xla of <tensorflow.python.eager.polymorphic_function.tracing_compiler.TfMethodTarget object at 0x7a404cf20190>> and will run it as-is.\n",
            "Cause: Unable to locate the source code of <bound method _BaseOptimizer._update_step_xla of <tensorflow.python.eager.polymorphic_function.tracing_compiler.TfMethodTarget object at 0x7a404cf20190>>. Note that functions defined in certain environments, like the interactive Python shell, do not expose their source code. If that is the case, you should define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.experimental.do_not_convert. Original error: could not get source code\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING: AutoGraph could not transform <bound method _BaseOptimizer._update_step_xla of <tensorflow.python.eager.polymorphic_function.tracing_compiler.TfMethodTarget object at 0x7a404cf20190>> and will run it as-is.\n",
            "Cause: Unable to locate the source code of <bound method _BaseOptimizer._update_step_xla of <tensorflow.python.eager.polymorphic_function.tracing_compiler.TfMethodTarget object at 0x7a404cf20190>>. Note that functions defined in certain environments, like the interactive Python shell, do not expose their source code. If that is the case, you should define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.experimental.do_not_convert. Original error: could not get source code\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "948/948 [==============================] - 28s 20ms/step - loss: 0.2990 - accuracy: 0.8704\n",
            "Epoch 2/5\n",
            "948/948 [==============================] - 10s 11ms/step - loss: 0.2198 - accuracy: 0.9145\n",
            "Epoch 3/5\n",
            "948/948 [==============================] - 6s 7ms/step - loss: 0.1957 - accuracy: 0.9218\n",
            "Epoch 4/5\n",
            "948/948 [==============================] - 10s 11ms/step - loss: 0.1865 - accuracy: 0.9293\n",
            "Epoch 5/5\n",
            "948/948 [==============================] - 7s 7ms/step - loss: 0.1742 - accuracy: 0.9320\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss, accuracy = model1.evaluate(testing_padded,y_test)\n",
        "\n",
        "print(f'Loss: {loss}')\n",
        "print(f'Accuracy: {accuracy}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QQuI7cZXLXbJ",
        "outputId": "b507a637-1c0b-461b-bad6-97c548c384df"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x7a404f0fdf30> and will run it as-is.\n",
            "Cause: Unable to locate the source code of <function Model.make_test_function.<locals>.test_function at 0x7a404f0fdf30>. Note that functions defined in certain environments, like the interactive Python shell, do not expose their source code. If that is the case, you should define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.experimental.do_not_convert. Original error: could not get source code\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x7a404f0fdf30> and will run it as-is.\n",
            "Cause: Unable to locate the source code of <function Model.make_test_function.<locals>.test_function at 0x7a404f0fdf30>. Note that functions defined in certain environments, like the interactive Python shell, do not expose their source code. If that is the case, you should define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.experimental.do_not_convert. Original error: could not get source code\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "316/316 [==============================] - 2s 5ms/step - loss: 0.1756 - accuracy: 0.9321\n",
            "Loss: 0.17557016015052795\n",
            "Accuracy: 0.9321329593658447\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPtBCechq9wzcUaahO1LE+2",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}