{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP1BdT3rtpaq6FapKG63zuR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hishamp3/Thesis-NLP/blob/main/SVM_baseline_approach1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "UqgRz2UB0v9L"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import matplotlib.pyplot as plt\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import pickle\n",
        "import time\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "id": "ngkUKZGu35uo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "531e4cfa-9bdc-445d-ae2f-9ebf5184a036"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv(\"./sample_data/fake reviews dataset.csv\",usecols=[\"text_\",\"label\"])"
      ],
      "metadata": {
        "id": "2-0rPISr2S3Q"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['fake']=df['label'].apply(lambda x: 1 if x=='CG' else 0)"
      ],
      "metadata": {
        "id": "HgqBHH1t2wHN"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cleanedData = []\n",
        "\n",
        "lemma = WordNetLemmatizer()\n",
        "swords = stopwords.words(\"english\")\n",
        "for text in df[\"text_\"]:\n",
        "\n",
        "    # Cleaning links\n",
        "    text = re.sub(r'http\\S+', '', text)\n",
        "\n",
        "    # Cleaning everything except alphabetical and numerical characters\n",
        "    text = re.sub(\"[^a-zA-Z0-9]\",\" \",text)\n",
        "\n",
        "    # Tokenizing and lemmatizing\n",
        "    text = nltk.word_tokenize(text.lower())\n",
        "    text = [lemma.lemmatize(word) for word in text]\n",
        "\n",
        "    # Removing stopwords\n",
        "    text = [word for word in text if word not in swords]\n",
        "\n",
        "    # Joining\n",
        "    text = \" \".join(text)\n",
        "\n",
        "    cleanedData.append(text)"
      ],
      "metadata": {
        "id": "CYXRUWZr3qA-"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(0,5):\n",
        "    print(cleanedData[i],end=\"\\n\\n\")"
      ],
      "metadata": {
        "id": "p_YjDoPq4dp_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc7c39dc-14c5-446d-8a87-e002b48800d2"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "love well made sturdy comfortable love pretty\n",
            "\n",
            "love great upgrade original mine couple year\n",
            "\n",
            "pillow saved back love look feel pillow\n",
            "\n",
            "missing information use great product price\n",
            "\n",
            "nice set good quality set two month\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = CountVectorizer(max_features=10000)\n",
        "BOW = vectorizer.fit_transform(cleanedData)"
      ],
      "metadata": {
        "id": "yx4lDwo54h7m"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "# X_train, X_test, y_train, y_test = train_test_split(df['text_'],df['fake'], stratify=df['fake'])\n",
        "x_train,x_test,y_train,y_test = train_test_split(BOW,np.asarray(df[\"fake\"]))"
      ],
      "metadata": {
        "id": "S9-L70TM2zF-"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(x_train.shape)\n",
        "print(x_test.shape)\n",
        "print(y_train.shape)\n",
        "print(y_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SMYW-OXr5rsh",
        "outputId": "c429d15e-cc78-4251-80a7-fda0a5701adb"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(30324, 10000)\n",
            "(10108, 10000)\n",
            "(30324,)\n",
            "(10108,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "start_time = time.time()\n",
        "\n",
        "model = SVC()\n",
        "model.fit(x_train,y_train)\n",
        "\n",
        "end_time = time.time()\n",
        "process_time = round(end_time-start_time,2)\n",
        "print(\"Fitting SVC took {} seconds\".format(process_time))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bjzv6nXa5x1x",
        "outputId": "b8c589c4-98fa-4722-9d0e-fe585cd8876d"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting SVC took 427.22 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = model.predict(x_test)"
      ],
      "metadata": {
        "id": "pPISIjVl56GH"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score,confusion_matrix\n",
        "\n",
        "print(\"Accuracy of model is {}%\".format(accuracy_score(y_test,predictions) * 100))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HUqa6YWE58FR",
        "outputId": "4fb7499c-4a5b-488d-c4de-cf442155efd7"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of model is 84.58646616541353%\n"
          ]
        }
      ]
    }
  ]
}